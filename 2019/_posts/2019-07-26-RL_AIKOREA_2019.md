---
layout: post
title:  "RL : AI KOREA 2019"
author: "LSM"
tag : [RL, AIKOREA-2019]
categories : [RL]
---

# AIKOREA 2019
## 알기 쉬운 강화 학습(4), 강의자 : 중앙대학교 김중헌 교수
- Supervised Learning
    - 라벨이 있는 상태로 학습이 진행
- Unsupervised Learning
    - 라벨이 없는 상태로 학습이 진행
    - clustering
    - dimensionality reduction
        - 커다란 상태로 표현된 벡터가 있음 -> Matrix Multiplication을 통해 차원을 줄임
        - 줄이는 방법 ex) PCA, LDA 등
- Reinforcement Learning
    - Sequence Making
    - Dynamic Programing
        - 스페이스에서 새 공간이 증가하더라도 공간이 optimal이면 다음 공간도 optimal임
        - ex) 피보나치 수열 (점화식 문제), 파스칼 삼각형, 냅색 문제
        - Polynoimal Time - 고려할 space가 늘어나면 시간이 linear하게 늘어남 => Exponential Gross

- Q-Learning
    - Q(s, a) = r + argmax_a Q(s', a')
    - 끝까지 가야 계산이 가능함
    - cf) Imitation Learning
        - 다른 agent가 하는 것을 보고 Reward를 찾아냄
        - Optimal은 아님
        - 다른 Agent의 행동을 따라하는 것이 목적
    - Discount Factor
        - 미래 가치를 감소 시킴
        - 빠르게 도착지에 도착할 수 있도록 함

- MDP
    - **Transition Function**은 MDP만의 요소, 나머지는 Q-learning과 동일
    - Transition Probability, Reward를 알게 되면 planning 문제가 됨
        - Reward 값 찾는 것이 가장 큰 문제
```
S : state
A : Action
R : Reward Function
T : Transition Function
γ : Discount Factor
```

### Imitation Learning
- ICML 2018에 발표한 기법
- 전문가가 시연한 것을 따라함
- RL의 문제점
    - Reward Shaping - 실제 문제에 적용하기에 어려움
        - 정교한 컨트롤 값을 찾아내는 것이 정말 어려움
        - 시뮬레이션에서는 잘 되나 실생활은 잘 안됨
    - Safe Learning
    - Exploration Process
    
    => 전문가가 하는 것을 보고 따라함으로 해결하고자 함
- 범용에서 쓰이는 것은 어렵지만 특색 있는 것에서 사용하는 것은 좋음
    - 자동차, 게임 등
- Inverse Reinforcement Learning
    - 전문가의 state, action을 가져와 역으로 Reward를 계산
    - Behavior Clonning
        - 전문가가 방문한 state를 가지고 있음
        - Optimal action과 뉴럴넷 policy의 loss를 가장 적게 하는 것을 목표
    - Interactive Expert
        - 학습한 것과 근사한 것의 loss를 가장 적게 하는 것을 목표
    - GAIL (**가장 Hot**)
        - RL + GAN
        - 최근의 imitation learning은 gail 기반임

- Random Search
    - 잘하면 편미분하지 않아도됨
    - 편미분은 병렬 처리 힘듦
    - 랜덤 서치를 잘하면 뉴럴넷하는 것과 비슷한 성능을 만들어 낼 수 있음

---

## 알기 쉬운 강화 학습(5), 강의자 : UNIST 최재식 교수
- 로봇 응용
    - **Model-Based RL**
        - 로봇 분야에서 가장 중점적으로 활용되고 있음
        - 인내심이 강하면 value-based model, 아니면 model-based

- Learning Manipulation Task
    - 입력 : 이미지
    - 출력 : 모터 토크
    - 일반적인 학습은 힘들기 때문에 **가이드**를 사용하자
        - Linear Controller  (이 부분만 강화학습)
    - ADMM
        - objective가 2개가 있음
        - 2개의 objective를 풀기 위해 loss function을 디자인
        - 패널티 텀을 넣음
    - BADMM
        - ADMM은 확률 함수를 표현하는 것이 아님
        - KL-Divergence를 사용해
        - 분포 나타냄
            - 분포로 나타내면 표현하고자 하는 trajectory를 줄일 수 있음
    - Baxter를 GPS(Guided Policy Search)와 연동

- Model-Agnostic Meta-RL
    - 적은 샘플에서 빠른 학습을 어떻게 할 수 있을까?
    - Learning to Learning
        - prioir experience를 가짐
        - 공통 요소에 대해 학습을 함
        - 모델을 Fine-Tuning
    - task를 나눠하면 적은 gradient로 fine-tunning 모델 보다 빠르게 할 수 있음
        - 계산양이 오래 걸림
        - Meta Overfit - 특정 task만 잘 되고 일반적인 task는 안되는 경우가 생김
    - 사람의 demonstration을 표현
        - 3D CNN을 이용해 샘플의 좋고 나쁨을 학습
        - 새로운 샘플 데이터에 대해 Demonstration feature가 더 많은 활성화를 할 수 있도록 만듦
    - 다른 것보다 사람 demo가 가장 유효

---
## 알기 쉬운 강화 학습(6), 강의자 : NCSOFT Game AI Lab 정지년 팀장, 오인석 연구원
### 게임과 강화학습
- 왜 게임을 사용할까?
    - 저렴한 시뮬레이션 비용
        - 실물건은 시간도 많이 들고 비용도 많이 들음
    - 안전성
        - 자동차의 경우 사고시 재산 손실 및 상해가 있을 수 있음
    - 실세계 못지 않은 복잡도가 있음
        - 게임을 통해 실세계에 연동될 수 있을 것이라 봄

- 게임은 왜 강화학습에 관심?
    - 기존의 게임 AI
        - Tree search, Hard-written rule 등의 고전적인 방법을 통해 만듦
        - 많은 연산이 소요되고, Exceptional Case가 많이 발생하기 때문에 디버깅에 많은 시간 소요
    - 고성능 게임 AI Agent를 머신러닝을 통해서 자동생성

### 상용 게임 강화학습 적용 사례
1. Quake3 Arena : Capture the flag
    - DeepMind에서 진행
    - 상대 진영에 있는 깃발을 뺏어 본인 진영에 가져가는 것 (팀플레이)
    - Agent 입장에서 바라보는 raw image를 이용해서 학습이 진행
    - Population based RL
        - Agent pool을 만들어서 자신과 비슷한 수준과 팀을 만들어 강화 시켜나는 방법

2. Dota 2
    - OpenAI에서 진행
    - 5:5 팀플레이가 가능 (MultiAgent RL)
    - 문제 특징
        - 한 게임이 길음
        - reward가 매우 늦게 나오기 때문에 어떤 action에서 기인한지 잘 모르게 됨
        - partial observed state
            - 보이지 않는 곳에서 정보를 얻을 수 없음
        - 선택지가 많기 때문에 action complexity, state complexity가 매우 큼
        - PPO를 이용해 학습
        - 개별 reward, 팀원 reward를 나눔
            - 초반은 개별 reward, 후반은 팀원 reward를 이용해 학습
        
3. Starcraft 2
    - 실시간 전략 게임으로 높은 복잡도를 가짐
    - Supervised Learning + Reinforcement Learning
    - Population-based
        - 비슷한 수준의 agent 끼리 모아 강화
    - 강화대상을 한정시킴
    - 강화학습 알고리즘 : IMPALA

### NCSOFT 강화학습 사례
1. 미니 RTS 강화학습 (2017)
    - reward : 게임의 승패
        - sparse한 게임 reward
        - 잘하는 상대와 싸우게 되면 학습을 하기 힘들 수 있음
        - time scale을 넣어 reward에 가중치를 줌

    - 어려운 점
        1. 실시간, 공간적 반응
            - VGG Network
        2. sparse 보상
            - Hierarchical RL
        3. 환경 복잡도
            - Hierarchical Architecture
    
    - 알고리즘
        - GA3C
            - 최근은 Asynchronous가 필요 없음
            - A2C만 해도 됨
            - GPU 학습이라 효율이 좋음

        - HRL
            - 희귀한 보상을 극복하기 위한 모델
            - Meta Network = Action Network + Position Network
                - Goal을 Meta Network에서 설정
                - Action Network는 Goal을 달성하기 위해 시뮬
        
        - HA
            - action들을 나눠 놓음
            - 해석이 좀 더 편해짐
            - 모듈 별로 학습 역시 가능해 사람이 컨트롤 하기 쉬워짐

2. Blade & Soul 비무
    - observation, action에 대한 정의
    - 100msec 안에 완료하기 위해 tree-search 등은 사용 불가
    - 실시간 => 신경망
    - 대전상대 풀을 만들어 상대를 계속 바꿔나가며 강화시킴
    - 통제 가능한 개성을 부여해 학습된 agent마다 다른 개성을 가지게 만듦
    - 문제 복잡도를 줄이는 방향으로 나아감

    - 학습 절차
        - ACER
            - Actor-Critic 관련 논문
            - off policy가 되어야 함
            - 상대가 계속적으로 바뀌기 때문에 stochastic policy으로 접근해야함
        - pool에 있는 상대에 있어서 강해지는 방향으로 학습하게 됨
        - pool에는 강한 상대가 있도록 함
    
    - Reward Shaping
        - 다양성을 의도가 가지고 만들 수 있어야 함
    
    - 문제의 복잡도
        - Macro Action
            - Moving Decision
        - Action Filtering
        - Feature 전처리