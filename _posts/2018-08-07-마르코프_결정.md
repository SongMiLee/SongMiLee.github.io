---
layout: post
title:  "마르코프 결정 과정"
author: "LSM"
categories : [til]
---
## Markov Decision Process
- 어떤 상태가 일정 간격으로 변함
- 다음 상태는 현재 상태에만 의존해 확률적으로 변하는 상태 변화
- p[st+1|st] = p[st+1|s1, s2, s3 ..... st]

cf) 마르코프 과정에서 시간 변화에 신경쓰지 않고 이산적 경우만 고려한 경우 **Markov Chain** 이라 불림
(각 시행 결과는 바로 직전의 시행 결과에만 영향을 받음)

### Discount Factor
- 감쇄계수
- 미래에 있을 보상 값을 현재 가치로 환산한 값 [0, 1]
- 값이 1이면 모든 미래 가치를 현재와 동일한 비중으로 봄

### Policy
- state와 action을 연결해주는 함수
- 상태 s가 주어졌을 때 행동 a를 할 확률 정의
- **제약 조건 : 정책은 시간의 흐름과 무관**

### 행동 가치 함수
- 상태 s에서 행동 a를 선택해 정책 pi를 따랐을 때 기대되는 가치 총합

### 상태 가치 함수
- 상태 s에서 정책 pi를 따랐을 때 기대되는 가치의 총합

**Markove Decision process는 상태 가치 함수가 가장 큰 정책을 찾는 것**

### Markov Property
- 의사결정을 위해 environment로 부터 정보를 받음
- 이 정보들의 특성을 markov property라 명명
- 이를 이용해 현재 시점에서 미래가치를 예측해 의사결정을 가능하게 함



